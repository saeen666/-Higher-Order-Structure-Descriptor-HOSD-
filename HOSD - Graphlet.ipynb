{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, copy, os, pickle, random\n",
    "from scipy.io import loadmat\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import base\n",
    "import sys\n",
    "import math\n",
    "%matplotlib inline\n",
    "#import ipyparallel as ipp\n",
    "import dill\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import OrderedDict \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, SVG, HTML\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "pd.options.display.max_columns = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_grid_search_random_forest():\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "                        min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, bootstrap=True, \n",
    "                                oob_score=False, n_jobs=6, verbose=0, warm_start=False,\n",
    "                                    max_depth = None,class_weight=\"balanced_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_connected_components(n=5):\n",
    "    CONNECTED_COMPONENTS_PATH   = 'graphs'\n",
    "    dirs = []\n",
    "    connected_graphs_components = {}\n",
    "    for i in os.listdir(CONNECTED_COMPONENTS_PATH):\n",
    "        if n >= int(i[3:]):\n",
    "            dirs.append(i)\n",
    "\n",
    "    for path in dirs:\n",
    "        for count,file in enumerate(os.listdir(os.path.join(CONNECTED_COMPONENTS_PATH, path))): \n",
    "            file_path = os.path.join(CONNECTED_COMPONENTS_PATH, path, file)\n",
    "            if file_path.endswith('.txt'): \n",
    "                connected_graph = np.loadtxt(file_path)\n",
    "                connected_graphs_components['g_'+path[3:]+'_'+str(count+1)] =  nx.from_numpy_array(np.array(connected_graph))\n",
    "    return connected_graphs_components           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_dataset(file_name):\n",
    "    #i = 'G_nci1'\n",
    "    dd = base.fetch_dataset(file_name)\n",
    "    graph_list = []\n",
    "    for gg in dd.data:\n",
    "        g_ = nx.Graph()\n",
    "        g_.add_edges_from([(i[0], i[1]) for i in gg[0]])\n",
    "        graph_list.append(g_)\n",
    "    data_y = dd.target\n",
    "    return graph_list, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_compressed_graph_whole(g1, value, primeN):\n",
    "    idx = 1\n",
    "    data = OrderedDict() \n",
    "    #combined = []\n",
    "    data[idx] = OrderedDict()\n",
    "    # check if list is empty\n",
    "    temp_graph = copy.copy(g1)\n",
    "    \n",
    "    gNew = nx.Graph()\n",
    "\n",
    "    for v in value:\n",
    "        #print(\" value:\", v)\n",
    "        gX = nx.Graph(g1.subgraph(v))\n",
    "        gNew.add_nodes_from(gX.nodes())\n",
    "        gNew.add_edges_from(gX.edges())\n",
    "\n",
    "    gNew.remove_edges_from(gNew.selfloop_edges())\n",
    "\n",
    "    avDegrees = []\n",
    "    connected_shapes = []\n",
    "\n",
    "    for gX in list(nx.connected_component_subgraphs(gNew)):\n",
    "        sum = 0\n",
    "        for key, value in gX.degree():\n",
    "            sum += value\n",
    "        sum /= len(gX.degree())\n",
    "        #print(sum)\n",
    "        avDegrees.append(sum)\n",
    "        connected_shapes.append(list(gX.nodes()))\n",
    "    \n",
    "    # contract\n",
    "    for i in connected_shapes:\n",
    "        #combined.append(i[0])\n",
    "        p = primeN        \n",
    "        for node in i[1:]:\n",
    "            temp_graph = nx.contracted_nodes(temp_graph, i[0], node, self_loops = False)\n",
    "            p = p * g1.node[node]['layer']\n",
    "        temp_graph.node[i[0]]['layer'] = p \n",
    "        \n",
    "    return temp_graph, avDegrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#connected_graphs_components  = return_connected_components(n=6)       \n",
    "def return_counts(g_, connected_graphs_components):\n",
    "    data = OrderedDict()  # all the data indices goes here\n",
    "    no_connected = OrderedDict() \n",
    "    count_dict = OrderedDict() \n",
    "\n",
    "    idx = 1\n",
    "\n",
    "    g1 = g_\n",
    "    #store indices of a connected graphs for a particular graph\n",
    "    data[idx] = OrderedDict() \n",
    "    #iterating through connected graphs found in round1\n",
    "    no_connected[idx] = OrderedDict() \n",
    "    #no_connected['before'] = OrderedDict() \n",
    "    count_dict = OrderedDict() \n",
    "    for name, graph in connected_graphs_components.items():\n",
    "        GM = nx.isomorphism.MultiGraphMatcher(g1,graph)\n",
    "        sub_nodes = list(GM.subgraph_isomorphisms_iter())\n",
    "        if len(sub_nodes)>0:\n",
    "            p = [list(i.keys()) for i in list(GM.subgraph_isomorphisms_iter())]\n",
    "            sub_nodes = np.unique(np.sort(p,axis=1),axis=0)\n",
    "        count_dict[name] = (sub_nodes)\n",
    "        \n",
    "    #print(\"count dict: \" ,count_dict)\n",
    "    counts = []\n",
    "    for key, v in count_dict.items():\n",
    "        counts.append(len(v))\n",
    "    \n",
    "    \n",
    "    #data = pd.DataFrame(counts).transpose()\n",
    "    #data_hist = counts.applymap(lambda x: len(x)) #contains histogram without merging connected shapes\n",
    "    \n",
    "    return counts, count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_graphs(connected_graphs_components):\n",
    "    i = 1\n",
    "    k = len(connected_graphs_components)\n",
    "    ky = 5\n",
    "    kx = math.ceil(k/ky)\n",
    "\n",
    "    plt.figure(figsize=(20, kx*ky))\n",
    "    for key, g in connected_graphs_components.items():\n",
    "        #plt.figure(i)\n",
    "        plt.subplot(kx, ky, i)\n",
    "        plt.title(key)\n",
    "        nx.draw(g)\n",
    "        i = i + 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_graph_stuff(graph, connected_graphs_components, label, indexKey):\n",
    "    #counts, count_dict = return_counts(g_, connected_graphs_components)\n",
    "\n",
    "    \n",
    "    data_hist1, count_dict1 = return_counts(graph, connected_graphs_components)\n",
    "\n",
    "    pbar2 = tqdm_notebook(range(len(count_dict1)), desc=\"Computing Sub-Histogram\")\n",
    "\n",
    "    compressedGs = OrderedDict() \n",
    "\n",
    "    for key, value in count_dict1.items():\n",
    "\n",
    "        currDict = OrderedDict() \n",
    "        data_histT = [0] * len(count_dict1)\n",
    "        data_histT2 = [0] * len(count_dict1)\n",
    "        \n",
    "        numSepCounts = 6\n",
    "        newnodes = 0\n",
    "        newedges = 0\n",
    "            \n",
    "        data_histT3 = [0] * len(count_dict1) * numSepCounts\n",
    "        avDegrees = []\n",
    "        \n",
    "        if (len(value) > 0):\n",
    "            g2, avDegrees = return_compressed_graph_whole(graph, value, 2)\n",
    "            newnodes = len(g2.nodes())\n",
    "            newedges = len(g2.edges())\n",
    "            \n",
    "            data_histT, count_dict2 = return_counts(g2, connected_graphs_components)\n",
    "            counts = []\n",
    "            separateCounts = {}\n",
    "            for i in range(numSepCounts):\n",
    "                separateCounts[i + 1] = []\n",
    "                \n",
    "                \n",
    "            for k, v in count_dict2.items():\n",
    "                thiscount = 0\n",
    "                # v is each \n",
    "                separateCountThis = {}\n",
    "                for i in range(numSepCounts):\n",
    "                    separateCountThis[i + 1] = 0\n",
    "                \n",
    "                # for each component present for this graphlet\n",
    "                for oneV in v:\n",
    "                    # one subset of connected components \n",
    "                    condition = False\n",
    "                    numLayered = 0;\n",
    "                    \n",
    "                    for oneNode in oneV:\n",
    "                        if (g2.node[oneNode]['layer'] == 2):\n",
    "                            numLayered += 1\n",
    "                            \n",
    "                    if (numLayered > 0):\n",
    "                        #print(oneV)\n",
    "                        thiscount += 1\n",
    "                        separateCountThis[numLayered] += 1\n",
    "                    \n",
    "                counts.append(thiscount)\n",
    "                for i in range(numSepCounts):\n",
    "                    separateCounts[i + 1].append(separateCountThis[i+1])\n",
    "\n",
    "            data_histT2 = counts\n",
    "            data_histT3 = []\n",
    "            for i in range(numSepCounts):\n",
    "                data_histT3 += separateCounts[i + 1]\n",
    "                    \n",
    "        #currDict['key'] = key\n",
    "\n",
    "        currDict['nodes'] = [newnodes]\n",
    "        currDict['edges'] = [newedges]\n",
    "        currDict['degreeList'] = avDegrees\n",
    "        currDict['HoG'] = data_histT\n",
    "        currDict['HoGN'] = data_histT2\n",
    "        currDict['HoGN2'] = data_histT3\n",
    "\n",
    "        compressedGs[key] = currDict\n",
    "\n",
    "        pbar2.update(1)\n",
    "\n",
    "\n",
    "    pbar2.close()\n",
    "\n",
    "\n",
    "    retDict = OrderedDict() \n",
    "    #retDict['graph'] = graph\n",
    "    retDict['nodes'] = len(graph.nodes())\n",
    "    retDict['edges'] = len(graph.edges())\n",
    "    retDict['class'] = label\n",
    "    retDict['key'] = indexKey\n",
    "    retDict['HoG'] = data_hist1\n",
    "    retDict['compressedGs'] = compressedGs\n",
    "\n",
    "    return retDict\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we have, in results, a list of graphs with various information\n",
    "#graph_item = results[0]\n",
    "\n",
    "#for graph_item in results:\n",
    "#    graph_item['compressedGs'][]\n",
    "def ret_minmax(results, key):\n",
    "\n",
    "    globMin = 110\n",
    "    globMax = 0\n",
    "    globMinG = 0\n",
    "    globMaxG = 0\n",
    "\n",
    "    #for key in connected_graphs_components.keys():\n",
    "    #key = 'g_6_55'\n",
    "    some = False\n",
    "    for graph_item in results:\n",
    "        if (len(graph_item['compressedGs'][key]['degreeList']) == 0):\n",
    "            continue\n",
    "        some = True\n",
    "        possMin = np.min(graph_item['compressedGs'][key]['degreeList'])\n",
    "        possMax = np.max(graph_item['compressedGs'][key]['degreeList'])\n",
    "\n",
    "        if (possMax > globMax):\n",
    "            globMax = possMax\n",
    "            #globMaxG = graph_item['graph']\n",
    "        if (possMin < globMin):\n",
    "            globMin = possMin\n",
    "            #globMinG = graph_item['graph']\n",
    "\n",
    "    #print(\" Max: \", globMax, \" Min: \", globMin)\n",
    "    glob = {}\n",
    "    if (some == True):\n",
    "        glob['min'] = globMin\n",
    "        glob['max'] = globMax\n",
    "    return glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from joblib import Parallel, delayed\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr  = return_connected_components(n=6)\n",
    "\n",
    "only_keep = ['g_3_1', 'g_4_2', 'g_5_21', 'g_6_24', \n",
    "             'g_4_1', 'g_5_1', 'g_6_1', \n",
    "             'g_3_2', 'g_4_4', 'g_5_6', 'g_6_55', \n",
    "             'g_4_6', 'g_5_14', 'g_6_16']\n",
    "             \n",
    "connected_graphs_components = OrderedDict()\n",
    "\n",
    "for key, val in curr.items():\n",
    "    if (key in only_keep):\n",
    "        connected_graphs_components[key] = curr[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_graphs(connected_graphs_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = \"PTC_MR\"\n",
    "graph_list, data_y = return_dataset(name)\n",
    "\n",
    "for g in graph_list:\n",
    "    nx.set_node_attributes(g, 1, 'layer')\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "results = Parallel(n_jobs=num_cores - 2, verbose=50)(delayed(\n",
    "    return_graph_stuff)(graph_list[key], connected_graphs_components, data_y[key], key) for key in range(len(graph_list)))\n",
    "pickle.dump( results, open( name + \"_G.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"PTC_MR\"\n",
    "results = pickle.load( open( name + \"_G.p\", \"rb\" ) )\n",
    "glob = {}\n",
    "\n",
    "for k in connected_graphs_components.keys():\n",
    "    retD = ret_minmax(results, k)\n",
    "    if (len(retD) > 0):\n",
    "        glob[k] = retD\n",
    "for k, v in glob.items():\n",
    "    v['max'] = math.ceil(v['max'])\n",
    "    v['min'] = math.floor(v['min'])\n",
    "\n",
    "dataX_1 = []\n",
    "dataX_2 = []\n",
    "dataX_3 = []\n",
    "dataX_4 = []\n",
    "\n",
    "binD = 10\n",
    "\n",
    "for graph_item in results:\n",
    "    data_item = copy.copy(graph_item['HoG'])\n",
    "    data_item2 = copy.copy(graph_item['HoG'])\n",
    "    data_item3 = copy.copy(graph_item['HoG'])\n",
    "    data_item4 = copy.copy(graph_item['HoG'])\n",
    "\n",
    "    data_item.append(graph_item['nodes'])\n",
    "    data_item2.append(graph_item['nodes'])\n",
    "    data_item3.append(graph_item['nodes'])\n",
    "    data_item4.append(graph_item['nodes'])\n",
    "\n",
    "    data_item.append(graph_item['edges'])\n",
    "    data_item2.append(graph_item['edges'])\n",
    "    data_item3.append(graph_item['edges'])\n",
    "    data_item4.append(graph_item['edges'])\n",
    "\n",
    "    for key, value in graph_item['compressedGs'].items():\n",
    "        if ((key not in glob.keys())):\n",
    "            continue\n",
    "\n",
    "        globMin = glob[key]['min']\n",
    "        globMax = glob[key]['max']\n",
    "\n",
    "        # set binwidth for this histogram\n",
    "        binW = binD*(globMax-globMin)\n",
    "        if (binW == 0):\n",
    "            binW = 1\n",
    "\n",
    "        # make histogram of degrees\n",
    "        degreeHist = [0] * binW\n",
    "        avDegrees = value['degreeList']\n",
    "        if (len(avDegrees) > 0):\n",
    "            #print(binW)\n",
    "            hist, bin_edges = np.histogram(avDegrees, bins=binW, range=(globMin, globMax), normed=None, weights=None, density=None)\n",
    "            degreeHist = hist.tolist()\n",
    "\n",
    "        data_item += degreeHist\n",
    "        data_item2 += degreeHist\n",
    "        data_item3 += degreeHist\n",
    "        data_item4 += degreeHist\n",
    "\n",
    "\n",
    "        data_item += value['nodes']\n",
    "        data_item2 += value['nodes']\n",
    "        data_item3 += value['nodes']\n",
    "        data_item4 += value['nodes']\n",
    "\n",
    "        data_item += value['edges']\n",
    "        data_item2 += value['edges']\n",
    "        data_item3 += value['edges']\n",
    "        data_item4 += value['edges']\n",
    "\n",
    "        thisHoGD = [x-y for x, y in zip(value['HoG'], value['HoGN'])] \n",
    "\n",
    "        if (not all(item >= 0 for item in thisHoGD)):\n",
    "            print(\"error\")\n",
    "\n",
    "        #data_item += thisHoGD\n",
    "        #data_item += value['HoGN2']\n",
    "\n",
    "        #data_item2 = copy.copy(data_item)\n",
    "        #data_item3 = copy.copy(data_item)\n",
    "        data_item += value['HoG']\n",
    "        data_item4 += value['HoG']\n",
    "\n",
    "        data_item2 += thisHoGD\n",
    "        data_item3 += thisHoGD\n",
    "        data_item4 += thisHoGD\n",
    "\n",
    "        data_item2 += value['HoGN']\n",
    "        data_item3 += value['HoGN2']\n",
    "\n",
    "\n",
    "        data_item4 += value['HoGN']\n",
    "        data_item4 += value['HoGN2']\n",
    "\n",
    "\n",
    "    dataX_1.append(data_item)\n",
    "    dataX_2.append(data_item2)\n",
    "    dataX_3.append(data_item3)\n",
    "    dataX_4.append(data_item4)\n",
    "\n",
    "\n",
    "dataX_1_ = np.array(dataX_1)\n",
    "dataX_2_ = np.array(dataX_2)\n",
    "dataX_3_ = np.array(dataX_3)\n",
    "dataX_4_ = np.array(dataX_4)\n",
    "\n",
    "data_X_1_ = dataX_1_[:,~np.all(dataX_1_==0,axis=0)]\n",
    "data_X_2_ = dataX_2_[:,~np.all(dataX_2_==0,axis=0)]\n",
    "data_X_3_ = dataX_3_[:,~np.all(dataX_3_==0,axis=0)]\n",
    "data_X_4_ = dataX_4_[:,~np.all(dataX_4_==0,axis=0)]\n",
    "\n",
    "if not os.path.exists(name):\n",
    "    os.makedirs(name)\n",
    "\n",
    "np.savetxt(name + '\\data_X_1_.txt',data_X_1_)\n",
    "np.savetxt(name + '\\data_X_2_.txt',data_X_2_)\n",
    "np.savetxt(name + '\\data_X_3_.txt',data_X_3_)\n",
    "np.savetxt(name + '\\data_X_4_.txt',data_X_4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(dataX_3_)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_X = data1\n",
    "data_y = np.array(myDataY)\n",
    "max_arr= []\n",
    "#,567,890,5678,78, 6,1122,101,11111,42\n",
    "for seed in [345]:\n",
    "    estimator  = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                            max_leaf_nodes=None, bootstrap=True, \n",
    "                                            oob_score=False, n_jobs=6,verbose=0, warm_start=False,\n",
    "                                            class_weight=None)\n",
    "\n",
    "    param_grid = {'n_estimators':[50,100,500], 'max_features':['sqrt'], 'min_samples_split':[2,3,4,5]}\n",
    "    kf = StratifiedKFold(n_splits=10, random_state = seed)\n",
    "    grid_rf    = GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=6, \n",
    "                 iid=False, refit=True, cv=kf, verbose=3, pre_dispatch='n_jobs', \n",
    "                 error_score='raise-deprecating')\n",
    "    #print(data_X.shape)\n",
    "    grid_rf.fit(data_X, data_y)\n",
    "    df = pd.DataFrame(grid_rf.grid_scores_)\n",
    "    max_arr.append(df['mean_validation_score'].max())\n",
    "    #grid_rf.cv_results_['mean_test_score']\n",
    "    #Pipeline2 = Pipeline([('drop_empty_column', dropCols(frac=1.0)),('grid_rf', grid_rf)])\n",
    "    #res = Pipeline2.fit(data_X, data_y)\n",
    "    print('Result computed for', filename, 'results are:',np.mean(max_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_rf.cv_results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
